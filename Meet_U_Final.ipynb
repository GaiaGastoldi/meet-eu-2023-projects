{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c68b404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from tabulate import tabulate\n",
    "from sklearn.cluster import KMeans\n",
    "from rdkit.Chem import MACCSkeys,MolFromSmiles\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20f132c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the csv file into dataframe\n",
    "df = pd.read_csv('pilot_library.csv')\n",
    "#display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eed088e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a8b36e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for RDKit molecule objects\n",
    "df['Molecule'] = df['smiles'].apply(Chem.MolFromSmiles)\n",
    "df['Fingerprint'] = df['Molecule'].apply(lambda x: MACCSkeys.GenMACCSKeys(x))\n",
    "fingerprints = np.array(df['Fingerprint'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e02314bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Calculate the Tanimoto distance to get the similarity matrix\\ndef calculate_tanimoto_similarity(fp1, fp2):\\n    intersection = np.sum(np.logical_and(fp1, fp2))\\n    union = np.sum(np.logical_or(fp1, fp2))\\n    if union == 0:\\n        return 0.0\\n    else:\\n        return intersection / union\\n\\nfingerprints = np.array(df['Fingerprint'].tolist())\\nsimilarity_matrix = np.zeros((len(fingerprints), len(fingerprints)))\\n\\nfor i in range(len(fingerprints)):\\n    for j in range(i, len(fingerprints)):\\n        tanimoto_similarity = calculate_tanimoto_similarity(fingerprints[i], fingerprints[j])\\n        similarity_matrix[i, j] = tanimoto_similarity\\n        similarity_matrix[j, i] = tanimoto_similarity\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Calculate the Tanimoto distance to get the similarity matrix\n",
    "def calculate_tanimoto_similarity(fp1, fp2):\n",
    "    intersection = np.sum(np.logical_and(fp1, fp2))\n",
    "    union = np.sum(np.logical_or(fp1, fp2))\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return intersection / union\n",
    "\n",
    "fingerprints = np.array(df['Fingerprint'].tolist())\n",
    "similarity_matrix = np.zeros((len(fingerprints), len(fingerprints)))\n",
    "\n",
    "for i in range(len(fingerprints)):\n",
    "    for j in range(i, len(fingerprints)):\n",
    "        tanimoto_similarity = calculate_tanimoto_similarity(fingerprints[i], fingerprints[j])\n",
    "        similarity_matrix[i, j] = tanimoto_similarity\n",
    "        similarity_matrix[j, i] = tanimoto_similarity\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff9052c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Create a 2D array representing the first 2 principle components\\npca = PCA(n_components=3)\\ndissimilarity_matrix = 1 - similarity_matrix\\npoints_2d = pca.fit_transform(similarity_matrix)\\npca_df = pd.DataFrame(\\n    data=points_2d, \\n    columns=['Principal Component 1','Principal Component 2','Principal Component 3'])\\ndisplay(pca_df)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Create a 2D array representing the first 2 principle components\n",
    "pca = PCA(n_components=3)\n",
    "dissimilarity_matrix = 1 - similarity_matrix\n",
    "points_2d = pca.fit_transform(similarity_matrix)\n",
    "pca_df = pd.DataFrame(\n",
    "    data=points_2d, \n",
    "    columns=['Principal Component 1','Principal Component 2','Principal Component 3'])\n",
    "display(pca_df)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2c8a316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"num_components_to_display = 3\\n\\n# Create a DataFrame with principal components and explained variance ratios\\ncomponents_df = pd.DataFrame({\\n    'Principal Component': [f'PC{i+1}' for i in range(num_components_to_display)],\\n    'Explained Variance Ratio': pca.explained_variance_ratio_[:num_components_to_display]\\n})\\n\\n# Display in tabular format\\ntable = tabulate(components_df, headers='keys', tablefmt='pretty', showindex=False)\\nprint(table)\\n\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"num_components_to_display = 3\n",
    "\n",
    "# Create a DataFrame with principal components and explained variance ratios\n",
    "components_df = pd.DataFrame({\n",
    "    'Principal Component': [f'PC{i+1}' for i in range(num_components_to_display)],\n",
    "    'Explained Variance Ratio': pca.explained_variance_ratio_[:num_components_to_display]\n",
    "})\n",
    "\n",
    "# Display in tabular format\n",
    "table = tabulate(components_df, headers='keys', tablefmt='pretty', showindex=False)\n",
    "print(table)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47295b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Plot the 2D array and coloring by cluster\\nplt.figure(figsize=(10, 8))\\nsns.scatterplot(x=points_2d[:, 0], y=points_2d[:, 1], hue=df['Cluster'], palette='viridis', alpha=0.7)\\nplt.title('Molecules in 2D Space with Kmeans Clustering')\\nplt.xlabel('Principal Component 1')\\nplt.ylabel('Principal Component 2')\\nplt.legend(title='Cluster')\\nplt.show()\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#K-mean Clustering\n",
    "num_clusters = 2 \n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "df['Cluster'] = kmeans.fit_predict(fingerprints)\n",
    "\n",
    "\"\"\"\n",
    "#Plot the 2D array and coloring by cluster\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x=points_2d[:, 0], y=points_2d[:, 1], hue=df['Cluster'], palette='viridis', alpha=0.7)\n",
    "plt.title('Molecules in 2D Space with Kmeans Clustering')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dc88d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 3D scatter plot with cluster colors\\nfig = plt.figure(figsize=(20, 20))\\nax = fig.add_subplot(111, projection='3d')\\n\\n# Use the first three principal components for x, y, and z axes\\nax.scatter(points_2d[:, 0], points_2d[:, 1], points_2d[:, 2], c=df['Cluster'], cmap='viridis', alpha=0.7)\\n\\nax.set_title('Molecules in 3D Space with K-Means Clustering')\\nax.set_xlabel('Principal Component 1')\\nax.set_ylabel('Principal Component 2')\\nax.set_zlabel('Principal Component 3')\\n\\nplt.show()\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#K-mean Clustering\n",
    "\n",
    "num_clusters = 2\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "df['Cluster'] = kmeans.fit_predict(fingerprints)\n",
    "\n",
    "\"\"\"\n",
    "# 3D scatter plot with cluster colors\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Use the first three principal components for x, y, and z axes\n",
    "ax.scatter(points_2d[:, 0], points_2d[:, 1], points_2d[:, 2], c=df['Cluster'], cmap='viridis', alpha=0.7)\n",
    "\n",
    "ax.set_title('Molecules in 3D Space with K-Means Clustering')\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_zlabel('Principal Component 3')\n",
    "\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbf1db0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1068\n"
     ]
    }
   ],
   "source": [
    "#Select 30 random molecules from the dataframe for the 2 clusters  \n",
    "group_1 = df[df['Cluster'] == 0]\n",
    "group_2 = df[df['Cluster'] == 1]\n",
    "\n",
    "print(len(group_1))\n",
    "\n",
    "random_1 = group_1.sample(n=30) \n",
    "random_2 = group_2.sample(n=30) \n",
    "\n",
    "Cluster_1 = pd.DataFrame(random_1)\n",
    "Cluster_2 = pd.DataFrame(random_2)\n",
    "\n",
    "#Save the chosen molecules into a csv file\n",
    "csv_1 = \"Cluster_1.csv\"\n",
    "csv_2 = \"Cluster_2.csv\"\n",
    "\n",
    "Cluster_1.to_csv(csv_1, index=False)\n",
    "Cluster_2.to_csv(csv_2, index=False)\n",
    "\n",
    "#display(group_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f699c2a",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5458216a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Adding the 3 known inhibitors into our dataset\n",
    "acetylcysteine_smiles = \"CC(=O)NC(CS)C(=O)O\"\n",
    "clavulanic_acid = \"C1C2N(C1=O)C(C(=CCO)O2)C(=O)O\"\n",
    "homovanillic_acid = \"COC1=C(C=CC(=C1)CC(=O)O)O\"\n",
    "c1 = pd.read_csv('Scores1.csv')\n",
    "c2 = pd.read_csv('Scores1.csv')\n",
    "\n",
    "def merge(c1,group_1):\n",
    "    merged_df = pd.merge(c1, df, left_on='Smile', right_on='smiles', how='left')\n",
    "    c1['fingerprint'] = merged_df['Fingerprint']\n",
    "\n",
    "    existing_names = set(c1['Name'])\n",
    "    existing_smiles = set(c1['Smile'])\n",
    "\n",
    "    filtered_group_1 = group_1[~(group_1['eos'].isin(existing_names) | group_1['smiles'].isin(existing_smiles))]\n",
    "    g_1 = filtered_group_1[['eos','smiles','Fingerprint']]\n",
    "\n",
    "    return c1, g_1\n",
    "\n",
    "c1 , g_1 = merge(c1,group_1)\n",
    "c2 , g_2 = merge(c2,group_2)\n",
    "display(len(c1))\n",
    "\n",
    "def add_compounds_to_dataframe(df, compounds, smiles_list):\n",
    "    rows = []\n",
    "    for compound, smiles in zip(compounds, smiles_list):\n",
    "        mol = MolFromSmiles(smiles)\n",
    "        fp = MACCSkeys.GenMACCSKeys(mol)\n",
    "        #print(len(fp))\n",
    "        new_row = {'eos': compound, 'smiles': smiles, 'Fingerprint': fp}\n",
    "        rows.append(new_row)\n",
    "    new_rows_df = pd.DataFrame(rows)\n",
    "    df = pd.concat([df, new_rows_df], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "g_1 = add_compounds_to_dataframe(g_1, ['Acetylcysteine', 'Clavulanic Acid', 'Homovanillic Acid'],[acetylcysteine_smiles, clavulanic_acid, homovanillic_acid])\n",
    "g_2 = add_compounds_to_dataframe(g_2, ['Acetylcysteine', 'Clavulanic Acid', 'Homovanillic Acid'],[acetylcysteine_smiles, clavulanic_acid, homovanillic_acid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0179596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Filter the dataframe using the sum of the fingerprints and taking into account only the one inferieur to 50\\ndef calculate_row_fingerprint_sum(row):\\n    fingerprint_sum = sum(row['Fingerprint'])\\n    return fingerprint_sum\\n\\ng_1['Fingerprint_Sum']  = g_1.apply(calculate_row_fingerprint_sum, axis=1)\\ng_2['Fingerprint_Sum']  = g_2.apply(calculate_row_fingerprint_sum, axis=1)\\n\\ng_1 = g_1[g_1['Fingerprint_Sum'] <= 50].copy()\\ng_2 = g_2[g_2['Fingerprint_Sum'] <= 50].copy()\\ndisplay(len(g_1))\\ndisplay(len(g_2))\\n#display(g_2)\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Filter the dataframe using the sum of the fingerprints and taking into account only the one inferieur to 50\n",
    "def calculate_row_fingerprint_sum(row):\n",
    "    fingerprint_sum = sum(row['Fingerprint'])\n",
    "    return fingerprint_sum\n",
    "\n",
    "g_1['Fingerprint_Sum']  = g_1.apply(calculate_row_fingerprint_sum, axis=1)\n",
    "g_2['Fingerprint_Sum']  = g_2.apply(calculate_row_fingerprint_sum, axis=1)\n",
    "\n",
    "g_1 = g_1[g_1['Fingerprint_Sum'] <= 50].copy()\n",
    "g_2 = g_2[g_2['Fingerprint_Sum'] <= 50].copy()\n",
    "display(len(g_1))\n",
    "display(len(g_2))\n",
    "#display(g_2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c9d08b",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e3f79aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mse = mean_squared_error(y, y_pred)\\nprint(\"Mean Squared Error:\", mse)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prediction(g_1,c1):\n",
    "    binary_fingerprints = [list(fp.ToBitString()) for fp in g_1['Fingerprint']]\n",
    "    fingerprint_features = pd.DataFrame(binary_fingerprints, columns=[f'bit_{i}' for i in range(len(binary_fingerprints[0]))])\n",
    "\n",
    "    bb = [list(fp.ToBitString()) for fp in c1['fingerprint']]\n",
    "    f = pd.DataFrame(bb, columns=[f'bit_{i}' for i in range(len(bb[0]))])\n",
    "\n",
    "    X = fingerprint_features\n",
    "    X_train = f\n",
    "    y_train = c1['Conf']\n",
    "    #print(y_train)\n",
    "\n",
    "    rf_model = RandomForestRegressor()\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    y_pred = rf_model.predict(X)\n",
    "\n",
    "    g_1['Conf'] = y_pred\n",
    "    return g_1\n",
    "\n",
    "pred_1 = prediction(g_1, c1)\n",
    "pred_2 = prediction(g_2, c2)\n",
    "\n",
    "#display(pred_1)\n",
    "# Evaluation\n",
    "\"\"\"mse = mean_squared_error(y, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4306f2",
   "metadata": {},
   "source": [
    "# Neual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21aab33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "124/124 [==============================] - 0s 2ms/step\n",
      "124/124 [==============================] - 0s 2ms/step\n",
      "124/124 [==============================] - 0s 2ms/step\n",
      "124/124 [==============================] - 0s 2ms/step\n",
      "124/124 [==============================] - 0s 2ms/step\n",
      "124/124 [==============================] - 0s 2ms/step\n",
      "124/124 [==============================] - 0s 2ms/step\n",
      "124/124 [==============================] - 0s 2ms/step\n",
      "124/124 [==============================] - 0s 2ms/step\n",
      "124/124 [==============================] - 0s 2ms/step\n",
      "124/124 [==============================] - 0s 2ms/step\n",
      "124/124 [==============================] - 0s 2ms/step\n",
      "124/124 [==============================] - 0s 2ms/step\n",
      "124/124 [==============================] - 0s 2ms/step\n",
      "124/124 [==============================] - 0s 2ms/step\n",
      "124/124 [==============================] - 0s 2ms/step\n",
      "124/124 [==============================] - 0s 2ms/step\n",
      "124/124 [==============================] - 0s 2ms/step\n",
      "124/124 [==============================] - 0s 2ms/step\n",
      "124/124 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "def prediction_neural_network(g_1, c1):\n",
    "    binary_fingerprints = [list(fp.ToBitString()) for fp in g_1['Fingerprint']]\n",
    "    fingerprint_features = pd.DataFrame(binary_fingerprints, columns=[f'bit_{i}' for i in range(len(binary_fingerprints[0]))])\n",
    "\n",
    "    bb = [list(fp.ToBitString()) for fp in c1['fingerprint']]\n",
    "    f = pd.DataFrame(bb, columns=[f'bit_{i}' for i in range(len(bb[0]))])\n",
    "    \n",
    "    X = fingerprint_features\n",
    "    X_train = f.astype('float32')\n",
    "    y_train = c1['Conf'].astype('float32').values.reshape(-1, 1)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Define a neural network model\n",
    "    model = Sequential([Dense(64, activation='relu', input_shape=(X_train.shape[1],)),Dense(32, activation='relu'),Dense(1)])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "    X = scaler.transform(X) \n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    g_1['Conf'] = y_pred.flatten()\n",
    "    return g_1\n",
    "\n",
    "# Prediction\n",
    "def prediction_neural_network_stable(g_1, c1, num_iterations=50):\n",
    "    all_predictions = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        y_pred = prediction_neural_network(g_1, c1)['Conf'].values\n",
    "        all_predictions.append(y_pred)\n",
    "\n",
    "    average_prediction = np.mean(all_predictions, axis=0)\n",
    "    g_1['Conf_Average'] = average_prediction\n",
    "\n",
    "    return g_1\n",
    "\n",
    "pred_1 = prediction_neural_network_stable(g_1, c1)\n",
    "pred_2 = prediction_neural_network_stable(g_2, c2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a642d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target(pred):\n",
    "    target = pred[pred['Conf_Average'] > -0.5]\n",
    "    return target\n",
    "\n",
    "target_1 = target(pred_1)\n",
    "target_2 = target(pred_2)\n",
    "display(len(target_1))\n",
    "display(len(target_2))\n",
    "display(target_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a5a8f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the target files in csv containing the molecules Confidance Average superior to -0.5\n",
    "csv_1 = \"Cluster_1_predictions.csv\"\n",
    "csv_2 = \"Cluster_2_predictions.csv\"\n",
    "\n",
    "target_1.to_csv(csv_1, index=True)\n",
    "target_2.to_csv(csv_2, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff296f36",
   "metadata": {},
   "source": [
    "# Evaluating the neural network on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29db0b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "WARNING:tensorflow:5 out of the last 128 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002BE2BFEB670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "WARNING:tensorflow:6 out of the last 129 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002BE2E620280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n"
     ]
    }
   ],
   "source": [
    "#Getting the predictions on just the molecules that have their DiffDock confidence alredy calculated\n",
    "def neural_network(c1, confidence_column='Conf'):\n",
    "    binary_fingerprints = [list(fp.ToBitString()) for fp in c1['fingerprint']]\n",
    "    fingerprint_features = pd.DataFrame(binary_fingerprints, columns=[f'bit_{i}' for i in range(len(binary_fingerprints[0]))])\n",
    "    \n",
    "    X = fingerprint_features\n",
    "    y = c1[confidence_column].astype('float32').values.reshape(-1, 1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = X_test.astype(float)\n",
    "    \n",
    "    model = Sequential([Dense(64, activation='relu', input_shape=(X_train.shape[1],)),Dense(32, activation='relu'),Dense(1)])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.squeeze(y_pred) \n",
    "\n",
    "    return y_pred,X_test\n",
    "\n",
    "# Prediction\n",
    "def stable(c1, num_iterations=50):\n",
    "    all_predictions = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        y_pred,X_test = neural_network(c1)#['Conf'].values\n",
    "        all_predictions.append(y_pred)\n",
    "\n",
    "    average_prediction = np.mean(all_predictions, axis=0)\n",
    "    result_df = pd.DataFrame({'eos': c1['Name'].iloc[X_test.index].values, \n",
    "    'SMILES': c1['Smile'].iloc[X_test.index].values,\n",
    "    'Conf': c1['Conf'].iloc[X_test.index].values,\n",
    "    'Average': average_prediction})\n",
    "    return result_df\n",
    "\n",
    "pred_1 = stable(c1)\n",
    "pred_2 = stable(c2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d382d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------+\n",
      "| Cluster   |    MAE |    MSE |\n",
      "+===========+========+========+\n",
      "| Cluster 1 | 0.3144 | 0.1278 |\n",
      "+-----------+--------+--------+\n",
      "| Cluster 2 | 0.3084 | 0.1138 |\n",
      "+-----------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "#Calculating the MSE and MEA on the predicted values\n",
    "mae1 = mean_absolute_error(pred_1['Conf'], pred_1['Average'])\n",
    "mse1 = mean_squared_error(pred_1['Conf'],  pred_1['Average'])\n",
    "\n",
    "mae2 = mean_absolute_error(pred_2['Conf'], pred_2['Average'])\n",
    "mse2 = mean_squared_error(pred_2['Conf'],  pred_2['Average'])\n",
    "mae1_formatted = f\"{mae1:.4f}\"\n",
    "mse1_formatted = f\"{mse1:.4f}\"\n",
    "mae2_formatted = f\"{mae2:.4f}\"\n",
    "mse2_formatted = f\"{mse2:.4f}\"\n",
    "\n",
    "data = [\n",
    "    [\"Cluster 1\", mae1_formatted, mse1_formatted],\n",
    "    [\"Cluster 2\", mae2_formatted, mse2_formatted]\n",
    "]\n",
    "headers = [\"Cluster\", \"MAE\", \"MSE\"]\n",
    "\n",
    "# Tabulate\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914c6f13",
   "metadata": {},
   "source": [
    "# Evaluating ou model on the given molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ddfa921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22:26:18] SMILES Parse Error: syntax error while parsing: OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1\n",
      "[22:26:18] SMILES Parse Error: Failed parsing SMILES 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1' for input: 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1'\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_12948\\171775106.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  r['MACCS_Fingerprint'] = r['SMILES'].apply(get_maccs_fp)\n"
     ]
    }
   ],
   "source": [
    "#Testing the neural network on the dataset given to us by the Prague team. Their top 100 molecules\n",
    "res = pd.read_csv('updated_repurposing_result.csv')\n",
    "r = res[['proteinID','SMILES','mean']]\n",
    "\n",
    "def get_maccs_fp(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        return MACCSkeys.GenMACCSKeys(mol)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "r['MACCS_Fingerprint'] = r['SMILES'].apply(get_maccs_fp)\n",
    "\n",
    "r = r.dropna(subset=['MACCS_Fingerprint'])\n",
    "#display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b30ef0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 65ms/step\n"
     ]
    }
   ],
   "source": [
    "#Prediction on the 100 molecules\n",
    "def prediction_neural(r):\n",
    "    binary_fingerprints = [list(fp.ToBitString()) for fp in r['MACCS_Fingerprint']]\n",
    "    fingerprint_features = pd.DataFrame(binary_fingerprints, columns=[f'bit_{i}' for i in range(len(binary_fingerprints[0]))])\n",
    "    \n",
    "    X = fingerprint_features\n",
    "    y = r['mean'].astype('float32').values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = X_test.astype(float)\n",
    "    #display(X_train.shape)\n",
    "    \n",
    "    model = Sequential([Dense(64, activation='relu', input_shape=(X_train.shape[1],)),Dense(32, activation='relu'),Dense(1)])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.squeeze(y_pred) \n",
    "\n",
    "    result_df = pd.DataFrame({'proteinID': r['proteinID'].iloc[X_test.index].values, \n",
    "    'SMILES': r['SMILES'].iloc[X_test.index].values,\n",
    "    'Mean':r['mean'].iloc[X_test.index].values,\n",
    "    'Average': y_pred.flatten()})\n",
    "    return result_df\n",
    "\n",
    "\n",
    "prediction = prediction_neural(r)\n",
    "prediction['M'] = prediction['Mean'] - prediction['Average']\n",
    "#display(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d468a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 3.0114934555604496\n",
      "The MSE 14.339772077774379\n"
     ]
    }
   ],
   "source": [
    "#Calculating the MSE and MEA on the predicted values\n",
    "mae = mean_absolute_error(prediction['Mean'], prediction['Average'])\n",
    "mse = mean_squared_error(prediction['Mean'],  prediction['Average'])\n",
    "print('Mean Absolute Error (MAE):', mae)\n",
    "print('The MSE',mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
